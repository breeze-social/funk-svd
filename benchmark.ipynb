{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "source": [
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "from funk_svd.dataset import fetch_ml_ratings\n",
    "from funk_svd.utils import _timer\n",
    "from funk_svd import SVD"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data from MovieLens 20M dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[MovieLens 20M Dataset Research Paper](\"http://files.grouplens.org/papers/harper-tiis2015.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "df = fetch_ml_ratings(variant='20m', verbose=True)\n",
    "print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "source": [
    "df.head()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "source": [
    "df.tail()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform a train/val/test split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 138,493 different users in the MovieLens20m dataset, each of them having rated at least 20 movies. Let's sample the 4 last ratings per user and randomly split them between validation and test sets. \n",
    "\n",
    "To do so, we need to query our DataFrame for each user and then select their 4 last ratings. With so much users it's naturally quite expensive... hopefully it's possible to parallelize it as iterations are independant, allowing us to save some time (especially if you have good computing ressources). I'm using an Intel Core i7-8565U CPU (4 physical cores) on a 16GB laptop.\n",
    "\n",
    "<img src=\"https://www.dlapiper.com/~/media/images/insights/publications/2015/warning.jpg?la=en&hash=6F2E30889FD9E0B11016A1712E6E583575717C54\" width=\"23\" align=\"left\">\n",
    "\n",
    "&nbsp; If you want to run this notebook with **Windows**, you won't be able to use `multiprocessing.Pool` because it's lacking `fork` method. For simplicity you can just do it sequentially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "source": [
    "@_timer(text='')\n",
    "def compute_val_test_mask(data, i, n_process, n_rate=4):\n",
    "    val_test_mask = []\n",
    "    users = data['u_id'].unique()\n",
    "    \n",
    "    for u_id in users:\n",
    "        u_subset = data[data['u_id'] == u_id].copy()\n",
    "        val_test_mask += u_subset.iloc[-n_rate:].index.tolist()\n",
    "        \n",
    "    print(f'Process {i} done in', end=' ')\n",
    "    return val_test_mask"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "source": [
    "users = df['u_id'].unique()\n",
    "\n",
    "seed = 3\n",
    "np.random.seed(seed)\n",
    "np.random.shuffle(users)\n",
    "\n",
    "n_process = 12\n",
    "pool = mp.Pool(processes=n_process)\n",
    "\n",
    "df_splitted = [\n",
    "    df.query('u_id.isin(@users_subset)')\n",
    "    for users_subset in np.array_split(users, n_process)\n",
    "]\n",
    "\n",
    "results = [\n",
    "    pool.apply_async(compute_val_test_mask, args=(data, i, n_process))\n",
    "    for i, data in zip(range(n_process), df_splitted)\n",
    "]\n",
    "\n",
    "results = [p.get() for p in results]\n",
    "val_test_mask = [item for sublist in results for item in sublist]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "source": [
    "train = df.drop(val_test_mask)\n",
    "val = df.loc[val_test_mask].sample(frac=0.5, random_state=seed)\n",
    "test = df.loc[val_test_mask].drop(val.index.tolist())"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's fit our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "source": [
    "svd = SVD(lr=0.001, reg=0.005, n_epochs=100, n_factors=15,\n",
    "          early_stopping=True, shuffle=False, min_rating=1, max_rating=5)\n",
    "\n",
    "svd.fit(X=train, X_val=val)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict test set and compute results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "pred = svd.predict(test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(test['rating'], pred))\n",
    "mae = mean_absolute_error(test['rating'], pred)\n",
    "\n",
    "print(f'Test RMSE: {rmse:.2f}')\n",
    "print(f'Test MAE:  {mae:.2f}')\n",
    "print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparison with Surprise library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "source": [
    "from surprise import Dataset\n",
    "from surprise import Reader\n",
    "from surprise import SVD"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Format data according Surprise way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "reader = Reader(rating_scale=(1, 5))\n",
    "\n",
    "trainset = Dataset.load_from_df(train[['u_id', 'i_id', 'rating']],\n",
    "                               reader=reader).build_full_trainset()\n",
    "\n",
    "testset = Dataset.load_from_df(test[['u_id', 'i_id', 'rating']], reader=reader)\n",
    "testset = testset.construct_testset(testset.raw_ratings)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the model with the same parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "svd = SVD(lr_all=.001, reg_all=0.005, n_epochs=46, n_factors=15, verbose=True)\n",
    "svd.fit(trainset)\n",
    "print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict test set and compute results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "pred = svd.test(testset)\n",
    "y_true = [p.r_ui for p in pred]\n",
    "y_hat = [p.est for p in pred]\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_true, y_hat))\n",
    "mae = mean_absolute_error(y_true, y_hat)\n",
    "\n",
    "print(f'Test RMSE: {rmse:.2f}')\n",
    "print(f'Test MAE:  {mae:.2f}')\n",
    "print()"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Accuracy performance is naturally equivalent, difference stands in the computation time, `Numba` allowing us to run more than 10 times faster than with cython.\n",
    "\n",
    "| Movielens 20M | RMSE   | MAE    | Time          |\n",
    "|:--------------|:------:|:------:|--------------:|\n",
    "| Surprise      |  0.88  |  0.68  | 10 min 40 sec |\n",
    "| Funk-svd      |  0.88  |  0.68  |        42 sec |"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
